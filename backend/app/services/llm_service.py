from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_cohere import CohereRerank
from typing import List, Dict, Any, Optional
import logging
import os
from app.core.config import get_settings

logger = logging.getLogger(__name__)


class LLMService:
    """LLM ì„œë¹„ìŠ¤ (ìš”ì•½, í‚¤ì›Œë“œ ì¶”ì¶œ, ë‹µë³€ ìƒì„±)"""

    def __init__(self):
        self.settings = get_settings()
        self.embeddings = OpenAIEmbeddings(
            model=self.settings.embedding_model,
            openai_api_key=self.settings.openai_api_key
        )
        self.llm = ChatOpenAI(
            model=self.settings.llm_model,
            temperature=0.3,
            openai_api_key=self.settings.openai_api_key
        )
        self.reranker = CohereRerank(
            model=self.settings.rerank_model,
            cohere_api_key=self.settings.cohere_api_key
        )

    def generate_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ ìž„ë² ë”© ìƒì„±"""
        try:
            logger.info(f"ðŸ”¹ Embedding generation - text length: {len(text)}")
            embedding = self.embeddings.embed_query(text)
            logger.info(f"âœ… Embedding generated - dimension: {len(embedding)}")
            return embedding
        except Exception as e:
            logger.error(f"âŒ Error generating embedding: {e}")
            raise

    def generate_summary(self, text: str, max_length: int = 100) -> str:
        """ë¬¸ì„œ í•œ ì¤„ ìš”ì•½ ìƒì„±"""
        try:
            logger.info(f"ðŸ”¹ Summary generation - text length: {len(text)}")
            from langchain_core.prompts import ChatPromptTemplate

            prompt = ChatPromptTemplate.from_messages([
                ("system", "ë‹¹ì‹ ì€ ë¬¸ì„œ ìš”ì•½ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. í•µì‹¬ë§Œ ë‹´ì•„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤."),
                ("human", """ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œ ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ë‹´ì•„ì£¼ì„¸ìš”.
ìµœëŒ€ {max_length}ìž ì´ë‚´ë¡œ ìž‘ì„±í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}

í•œ ì¤„ ìš”ì•½:""")
            ])

            chain = prompt | self.llm
            response = chain.invoke({"text": text[:2000], "max_length": max_length})
            summary = response.content.strip()

            logger.info(f"âœ… Summary generated - length: {len(summary)}")
            return summary
        except Exception as e:
            logger.error(f"âŒ Error generating summary: {e}")
            raise

    def extract_keywords(self, text: str, max_keywords: int = 5) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            from langchain_core.prompts import ChatPromptTemplate

            prompt = ChatPromptTemplate.from_messages([
                ("system", "ë‹¹ì‹ ì€ í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."),
                ("human", """ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
ê²€ìƒ‰ì— ìœ ìš©í•œ ë‹¨ì–´ë¥¼ {max_keywords}ê°œ ì´í•˜ë¡œ ì„ íƒí•˜ì„¸ìš”.
í‚¤ì›Œë“œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ë‚˜ì—´í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}

í‚¤ì›Œë“œ:""")
            ])

            keyword_llm = ChatOpenAI(
                model=self.settings.llm_model,
                temperature=0.2,
                openai_api_key=self.settings.openai_api_key
            )

            chain = prompt | keyword_llm
            response = chain.invoke({"text": text[:1500], "max_keywords": max_keywords})

            keywords_text = response.content.strip()
            keywords = [k.strip() for k in keywords_text.split(",")]
            return keywords[:max_keywords]
        except Exception as e:
            logger.error(f"Error extracting keywords: {e}")
            raise

    def rerank_results(
        self,
        query: str,
        documents: List[Dict[str, Any]],
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """Cohere Rerankë¡œ ê²°ê³¼ ìž¬ì •ë ¬ + evidence ì¶”ì¶œ"""
        try:
            from langchain_core.documents import Document

            # Cohere rerankìš© ë¬¸ì„œ ì¤€ë¹„
            doc_texts = [
                f"{doc['summary']}\n{doc.get('preview', '')}"
                for doc in documents
            ]

            # LangChain Documentsë¡œ ë³€í™˜
            langchain_docs = [Document(page_content=text) for text in doc_texts]

            # Rerank ì‹¤í–‰
            reranked_docs = self.reranker.compress_documents(
                documents=langchain_docs,
                query=query
            )

            # ê²°ê³¼ ìž¬êµ¬ì„±
            reranked = []
            for i, doc in enumerate(reranked_docs[:top_k]):
                # ì›ë³¸ ë¬¸ì„œ ì°¾ê¸°
                original_index = doc_texts.index(doc.page_content)
                original_doc = documents[original_index]

                # evidence ìƒì„±
                evidence = self._extract_evidence(query, original_doc)

                reranked.append({
                    **original_doc,
                    "rerank_score": getattr(doc.metadata, 'relevance_score', 0.0),
                    "evidence": evidence
                })

            return reranked

        except Exception as e:
            logger.error(f"Error reranking: {e}")
            raise

    def _extract_evidence(self, query: str, document: Dict[str, Any]) -> str:
        """ê°„ë‹¨í•œ evidence ì¶”ì¶œ (ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¶€ë¶„ ì°¾ê¸°)"""
        summary = document.get("summary", "")
        preview = document.get("preview", "")

        # ì¿¼ë¦¬ ë‹¨ì–´ë“¤
        query_words = set(query.lower().split())

        # summaryì—ì„œ ë§¤ì¹­ë˜ëŠ” ë¶€ë¶„ ì°¾ê¸°
        if any(word in summary.lower() for word in query_words):
            return f"ìš”ì•½ì—ì„œ '{query}' ê´€ë ¨ ë‚´ìš© í¬í•¨"

        # previewì—ì„œ ë§¤ì¹­ë˜ëŠ” ë¶€ë¶„ ì°¾ê¸°
        for sentence in preview.split("."):
            if any(word in sentence.lower() for word in query_words):
                return sentence.strip()[:100] + "..."

        return "ë¬¸ì„œ ë‚´ìš©ê³¼ ê´€ë ¨ì„± ìžˆìŒ"

    def compose_answer(
        self,
        query: str,
        source_documents: List[Dict[str, Any]],
        max_docs: int = 3
    ) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        try:
            # ìƒìœ„ ë¬¸ì„œë“¤ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_parts = []
            for i, doc in enumerate(source_documents[:max_docs], 1):
                context_parts.append(
                    f"[ë¬¸ì„œ {i}] {doc['summary']}\n{doc.get('preview', '')[:300]}"
                )

            context = "\n\n".join(context_parts)

            from langchain_core.prompts import ChatPromptTemplate

            prompt = ChatPromptTemplate.from_messages([
                ("system", "ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ìƒì„± ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤."),
                ("human", """ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ë¬¸ì„œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìž‘ì„±í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {query}

ê´€ë ¨ ë¬¸ì„œë“¤:
{context}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
1. ë‹µë³€: 2-6ë¬¸ìž¥ìœ¼ë¡œ í•µì‹¬ ë‚´ìš© ìš”ì•½
2. í•µì‹¬ í¬ì¸íŠ¸: 3ê°€ì§€ ì¤‘ìš”í•œ ì ì„ ê°„ê²°í•˜ê²Œ

ë‹µë³€:""")
            ])

            answer_llm = ChatOpenAI(
                model=self.settings.llm_model,
                temperature=0.5,
                openai_api_key=self.settings.openai_api_key
            )

            chain = prompt | answer_llm
            response = chain.invoke({"query": query, "context": context})

            answer_text = response.content.strip()

            # ë‹µë³€ê³¼ í•µì‹¬ í¬ì¸íŠ¸ ë¶„ë¦¬
            parts = answer_text.split("í•µì‹¬ í¬ì¸íŠ¸")
            main_answer = parts[0].replace("ë‹µë³€:", "").strip()

            # í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ
            highlights = []
            if len(parts) > 1:
                highlight_text = parts[1].strip()
                for line in highlight_text.split("\n"):
                    line = line.strip()
                    if line and not line.startswith("í•µì‹¬"):
                        # ë¶ˆë¦¿ í¬ì¸íŠ¸ ì œê±°
                        clean_line = line.lstrip("â€¢-123456789. ").strip()
                        if clean_line:
                            highlights.append(clean_line)

            # ìµœëŒ€ 3ê°œë¡œ ì œí•œ
            highlights = highlights[:3]

            # ê¸°ë³¸ê°’ ì„¤ì •
            if not highlights:
                highlights = ["ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€", "ê´€ë ¨ ì •ë³´ í¬í•¨", "ì¶”ê°€ ê²€ìƒ‰ ê°€ëŠ¥"]

            return {
                "answer": main_answer,
                "highlights": highlights,
                "source_documents": source_documents[:max_docs]
            }

        except Exception as e:
            logger.error(f"Error composing answer: {e}")
            raise


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_llm_service = None


def get_llm_service() -> LLMService:
    global _llm_service
    if _llm_service is None:
        _llm_service = LLMService()
    return _llm_service
